# -*- coding: utf-8 -*-
"""Model NLP QA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J8xsXkaJF_d_Hs9ftwn8CpLzmt2Yv4IY
"""

# Install versi PyTorch yang stabil di Colab (cu118 untuk CUDA 11.8)
# !pip install torch==2.1.0+cu118 torchvision==0.16.0+cu118 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118
!pip install transformers datasets sentencepiece

import torch
import transformers

print("Torch version:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())
print("Transformers version:", transformers.__version__)

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

url = 'https://raw.githubusercontent.com/bebekgarut/Model-NLP-QA/main/Quran_R1_excel.xlsx'

df = pd.read_excel(url)

print(df.head())

df = df.drop(columns=['Unnamed: 0'])

print(df.head())

pip install datasets

from datasets import Dataset

dataset = Dataset.from_pandas(df)

print(dataset[0])

print(dataset.features)

from transformers import AutoTokenizer


tokenizer = AutoTokenizer.from_pretrained("t5-small")
def preprocess(example):
    inputs = f"question: {example['Question']} context: {example['Complex_CoT']}"

    model_inputs = tokenizer(inputs, padding="max_length", truncation=True, max_length=512)

    labels = tokenizer(example["Response"], padding="max_length", truncation=True, max_length=512)

    model_inputs["labels"] = labels["input_ids"]

    return model_inputs

tokenized_dataset = dataset.map(preprocess)

# Split data 90% training dan 10% testing
dataset_split = tokenized_dataset.train_test_split(test_size=0.1)
train_dataset = dataset_split["train"]
eval_dataset = dataset_split["test"]

from transformers import T5ForConditionalGeneration

model = T5ForConditionalGeneration.from_pretrained("t5-small")

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=2e-4,
    per_device_train_batch_size=4,
    num_train_epochs=20,
    weight_decay=0.01,
    save_total_limit=1,
    logging_dir="./logs",
    push_to_hub=False
)

from transformers import Trainer, DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator
)

import os
os.environ["WANDB_DISABLED"] = "true"

trainer.train()

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer
)

eval_result = trainer.evaluate()
print(eval_result)

sample = eval_dataset[0]
question = sample["Question"]
context = sample["Complex_CoT"]

input_text = f"question: {question} context: {context}"

inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True).to(model.device)

outputs = model.generate(**inputs, max_new_tokens=100)
answer = tokenizer.decode(outputs[0], skip_special_tokens=True)

print("Pertanyaan:", question)
print("Jawaban Prediksi:", answer)
print("Jawaban Asli:", sample["Response"])

# pip install rouge_score

import evaluate

rouge = evaluate.load("rouge")
bleu = evaluate.load("bleu")

predictions = []
references = []

for sample in eval_dataset:
    question = sample["Question"]
    context = sample["Complex_CoT"]
    input_text = f"question: {question} context: {context}"

    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, padding=True).to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=100)
    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)

    predictions.append(pred)
    references.append(sample["Response"])

rouge_result = rouge.compute(predictions=predictions, references=references)
print("ðŸ”¹ ROUGE Scores:")
for k, v in rouge_result.items():
    print(f"{k}: {v:.4f}")

bleu_result = bleu.compute(
    predictions=predictions,
    references=[[ref] for ref in references]
)
print("\nðŸ”¹ BLEU Score:")
print(f"BLEU: {bleu_result['bleu']:.4f}")

model.save_pretrained("/content/drive/MyDrive/ModelQnA/Model")
tokenizer.save_pretrained("/content/drive/MyDrive/ModelQnA/Tokenize")